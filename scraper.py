#!/usr/bin/env python3
import requests
from bs4 import BeautifulSoup, NavigableString
import json
import urllib.parse
import time
from pathlib import Path
import base64

# === üü¢ –í–≤–æ–¥–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ ===
CATEGORY_URL = "https://gorgia.ge/ka/bagi/"
PAGE_NUMBER = 4
START_ID = 91

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ===
BASE_URL = "https://gorgia.ge"
IMGBB_API_KEY = "16ccb20b7d07ea5522785bbda2a2ca64"  # –∫–ª—é—á imgbb
SAVE_DIR = Path("images_temp")
SAVE_DIR.mkdir(exist_ok=True)

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36"
}

# === –ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ (–≥—Ä—É–∑–∏–Ω—Å–∫–∏–π ‚Üí —Ä—É—Å—Å–∫–∏–π) ===
def translate_text(text, target_lang="ru"):
    if not text:
        return ""
    try:
        r = requests.get(
            "https://translate.googleapis.com/translate_a/single",
            params={
                "client": "gtx",
                "sl": "ka",
                "tl": target_lang,
                "dt": "t",
                "q": text,
            },
            timeout=10,
        )
        if r.status_code == 200:
            result = r.json()
            return "".join([t[0] for t in result[0]])
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
    return text

# === –ü–æ–ª—É—á–µ–Ω–∏–µ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã ===
def get_soup(url):
    r = requests.get(url, headers=HEADERS, timeout=20)
    r.raise_for_status()
    return BeautifulSoup(r.text, "html.parser")

# === –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Å—Å—ã–ª–∫–∏ –≤ webp ===
def convert_to_webp_url(old_url):
    if not old_url:
        return None
    return (
        old_url
        .replace("/images/thumbnails/240/240/", "/images/ab__webp/thumbnails/1100/900/")
        .replace(".jpg", "_jpg.webp")
        .replace(".JPG", "_jpg.webp")
        .replace(".jpeg", "_jpg.webp")
        .replace(".png", "_jpg.webp")
    )

# === –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ñ–æ—Ç–æ ===
def download_image(url, out_path):
    try:
        r = requests.get(url, headers=HEADERS, stream=True, timeout=20)
        if r.status_code == 200:
            with open(out_path, "wb") as f:
                for chunk in r.iter_content(1024 * 32):
                    f.write(chunk)
            return True
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {url}: {e}")
    return False

# === –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ imgbb ===
def upload_to_imgbb(image_path):
    try:
        with open(image_path, "rb") as f:
            img_base64 = base64.b64encode(f.read())
        r = requests.post(
            "https://api.imgbb.com/1/upload",
            data={"key": IMGBB_API_KEY, "image": img_base64},
            timeout=30
        )
        if r.status_code == 200:
            data = r.json()
            return data["data"]["url"]
        else:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ imgbb: {r.text}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –Ω–∞ imgbb: {e}")
    return None

# === –ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ä—Ç–æ—á–µ–∫ —Ç–æ–≤–∞—Ä–æ–≤ ===
def parse_page(category_url, page_number=1, start_id=1):
    if page_number > 1:
        url = f"{category_url}?page={page_number}" if category_url.endswith("/") else f"{category_url}/?page={page_number}"
    else:
        url = category_url

    print(f"\nüîó –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É: {url}")
    soup = get_soup(url)
    cards = soup.select(".ut2-gl__body")
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(cards)} —Ç–æ–≤–∞—Ä–æ–≤\n")

    products = []

    for i, card in enumerate(cards, start=start_id):
        title_tag = card.select_one(".ut2-gl__name a")
        title = title_tag.get_text(strip=True) if title_tag else ""
        link = urllib.parse.urljoin(BASE_URL, title_tag["href"]) if title_tag else None

        # === –¶–µ–Ω–∞ ===
        price_tag = card.select_one(".ty-price-num")
        price = None
        if price_tag:
            first_text = None
            for child in price_tag.children:
                if isinstance(child, NavigableString):
                    txt = str(child).strip()
                    if txt:
                        first_text = txt
                        break
            if not first_text:
                for s in price_tag.find_all('sup'):
                    s.extract()
                first_text = price_tag.get_text(strip=True)
            if first_text:
                try:
                    num_str = first_text.replace(",", ".").replace("‚Çæ", "").strip()
                    price = int(float(num_str))
                except ValueError:
                    price = None

        # === –§–æ—Ç–æ ===
        img_tag = card.select_one(".ut2-gl__image img")
        image_url = urllib.parse.urljoin(BASE_URL, img_tag["src"]) if img_tag else None
        webp_url = convert_to_webp_url(image_url)

        # === –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è ===
        stock_tag = card.select_one(".ty-qty-in-stock")
        availability = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
        in_stock = None
        if stock_tag:
            stock_text = stock_tag.get_text(strip=True)
            if "·Éõ·Éê·É†·Éê·Éí·É®·Éò·Éê" in stock_text:
                availability = "–í –Ω–∞–ª–∏—á–∏–∏"
                in_stock = True
            elif "·Éõ·Éê·É†·Éê·Éí·Éò ·Éò·É¨·É£·É†·Éî·Éë·Éê" in stock_text:
                availability = "–ù–µ—Ç –≤ –Ω–∞–ª–∏—á–∏–∏"
                in_stock = False

        # === –ü–µ—Ä–µ–≤–æ–¥ ===
        title_ru = translate_text(title)
        description_ru = translate_text(
            card.select_one(".product-description").get_text(strip=True)
            if card.select_one(".product-description") else ""
        )

        # === –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–æ—Ç–æ ===
        new_image_url = None
        if webp_url:
            filename = Path(webp_url.split("/")[-1])
            temp_path = SAVE_DIR / filename
            if download_image(webp_url, temp_path):
                uploaded = upload_to_imgbb(temp_path)
                if uploaded:
                    new_image_url = uploaded
                    print(f"üì∏ –ó–∞–≥—Ä—É–∂–µ–Ω–æ ‚Üí {new_image_url}")
            time.sleep(0.5)

        products.append({
            "id": i,
            "category": "–°–∞–¥",
            "title": title_ru,
            "price": price,
            "description": description_ru,
            "availability": availability,
            "in_stock": in_stock,
            "image_url": new_image_url or webp_url,
            "link": link
        })

        print(f"{i:03d}. {title_ru} ‚Äî {price} ‚Çæ ‚Äî {availability}")
        time.sleep(1)

    return products

# === –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—É—Å–∫ ===
if __name__ == "__main__":
    products = parse_page(CATEGORY_URL, PAGE_NUMBER, START_ID)
    output_name = f"gorgia_page_{PAGE_NUMBER}.json"
    with open(output_name, "w", encoding="utf-8") as f:
        json.dump(products, f, ensure_ascii=False, indent=2)
    print(f"\n‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(products)} —Ç–æ–≤–∞—Ä–æ–≤ –≤ {output_name}")
